# Model arguments
model_name_or_path: HuggingFaceTB/SmolVLM-500M-Instruct
# tokenizer_name_or_path: HuggingFaceTB/SmolVLM-500M-Instruct
model_revision: main
torch_dtype: bfloat16
attn_implementation: flash_attention_2
use_liger: false
bf16: true
tf32: true
output_dir: runs/HuggingFaceTB/SmolVLM-500M-Instruct-8bit_exp2

# Dataset arguments
dataset_id_or_path: HuggingFaceM4/ChartQA #"s3://sagemaker-ap-south-1-211125369293/datasets/text-to-sql/train_dataset.json"    #train_dataset.json
max_seq_length: 1024
packing: true

# LoRA arguments
use_peft: true
load_in_4bit: false
load_in_8bit: true
lora_target_modules: ['down_proj','o_proj','k_proj','q_proj','gate_proj','up_proj','v_proj']
# important as we need to train the special tokens for the chat template of llama 
# lora_modules_to_save: ["lm_head", "embed_tokens"] # you might need to change this for qwen or other models
lora_r: 8
lora_alpha: 8

# Training arguments
num_train_epochs: 2
per_device_train_batch_size: 2
gradient_accumulation_steps: 2
gradient_checkpointing: true
gradient_checkpointing_kwargs:
  use_reentrant: false
learning_rate: 2.0e-4 
lr_scheduler_type: constant
warmup_ratio: 0.1
dataset_text_field: ""
dataset_kwargs: {"skip_prepare_dataset": True}
evaluation_strategy: "steps"
eval_steps: 10            

# Logging arguments
logging_strategy: steps
logging_steps: 5
report_to:
- tensorboard
save_strategy: "epoch"
seed: 42

# Hugging Face Hub 
push_to_hub: False
